{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0ab5832-0a47-47e4-a7b2-36c7d789893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc07807-be75-4671-b38f-292864ad5056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b794ce88-198e-4197-9c1b-6392a94a7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"instruction-data-with-response.json\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f53127d1-9686-47f7-8558-26f040800eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have personal preferences or taste buds, but I can suggest some popular ice cream flavors that people enjoy:\n",
      "\n",
      "1. Vanilla\n",
      "2. Chocolate\n",
      "3. Strawberry\n",
      "4. Cookies and Cream\n",
      "5. Mint Chocolate Chip\n",
      "\n",
      "What's your favorite ice cream flavor?\n"
     ]
    }
   ],
   "source": [
    "def query_model(prompt, model=\"llama3.2\", url=\"http://localhost:11434/api/chat\"):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": {   \n",
    "            \"seed\": 123,\n",
    "            \"temperature\": 0,\n",
    "            \"num_ctx\": 2048\n",
    "        }\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    \n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "\n",
    "    return response_data\n",
    "\n",
    "\n",
    "model = \"llama3.2\"\n",
    "result = query_model(\"what is your favorite ice cream flavor?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d461fe7-7afe-4f29-90f2-febb3f81ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "Score:\n",
      ">> I would rate the model response \"The car is as fast as a cheetah.\" a score of 80 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response uses a simile, which is a common literary device used to compare two things.\n",
      "* The comparison is between the speed of the car and that of a cheetah, which is known for its incredible speed.\n",
      "* However, the model response could be improved by making the comparison more vivid and evocative. For example, \"The car is as fast as lightning\" creates a stronger image in the reader's mind.\n",
      "\n",
      "Overall, while the model response is not bad, it could benefit from a bit more creativity and precision to make it truly effective.\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> I would rate the model response a 20.\n",
      "\n",
      "The reason for this low score is that the model response contains an error in its classification of clouds. Cumulonimbus clouds are indeed associated with thunderstorms, but cumulus clouds are typically associated with fair weather and are often seen on warm, sunny days. The correct term should be \"cumulonimbus\" instead of \"cumulus\".\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> ### Input\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "### Output\n",
      "Jane Austen.\n",
      "\n",
      "### Score: 100/100\n",
      "\n",
      "The response is complete, accurate, and concise, making it a perfect score. The model has correctly identified the author of the novel \"Pride and Prejudice\" as Jane Austen.\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b019d13-2808-4930-8f45-efbf6915d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "Score:\n",
      ">> 4\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud associated with thunderstorms is a cumulus cloud.\n",
      "\n",
      "Score:\n",
      ">> 3\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> 5\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"You are an evaluation model.\"\n",
    "        f\"You will be given three inputs:\"\n",
    "        f\"1. The original prompt.\"\n",
    "        f\"2. The correct response.\"\n",
    "        f\"3. The generated response.\"\n",
    "        f\"Your task is to compare the generated response with the correct response and assign a rating from 1 to 5, based on how accurate and close the generated response is to the correct response.\"\n",
    "        f\"Scoring guidelines:\"\n",
    "        f\"5 = Perfectly accurate or nearly identical\"\n",
    "        f\"4 = Mostly correct, minor issues\"\n",
    "        f\"3 = Partially correct, significant issues\"\n",
    "        f\"2 = Mostly incorrect, small resemblance\"\n",
    "        f\"1 = Completely wrong or irrelevant\"\n",
    "        f\"Important rules:\"\n",
    "        f\"- Respond with ONLY the integer rating (1, 2, 3, 4, or 5).\"\n",
    "        f\"- Do NOT provide explanations, reasoning, or extra words.\"\n",
    "        f\"original prompt: {format_input(entry)}\"\n",
    "        f\"correct response: {entry['output']}\"\n",
    "        f\"generated response: {entry['model_response']}\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86184069-0014-446a-a4b3-4cafa0803ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|█████████████████████████████████████████████████████████████| 110/110 [04:43<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 3.54\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_model_scores(json_data, json_key, model=\"llama3.2\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "        f\"You are an evaluation model.\"\n",
    "        f\"You will be given three inputs:\"\n",
    "        f\"1. The original prompt.\"\n",
    "        f\"2. The correct response.\"\n",
    "        f\"3. The generated response.\"\n",
    "        f\"Your task is to compare the generated response with the correct response and assign a rating from 1 to 5, based on how accurate and close the generated response is to the correct response.\"\n",
    "        f\"Scoring guidelines:\"\n",
    "        f\"5 = Perfectly accurate or nearly identical\"\n",
    "        f\"4 = Mostly correct, minor issues\"\n",
    "        f\"3 = Partially correct, significant issues\"\n",
    "        f\"2 = Mostly incorrect, small resemblance\"\n",
    "        f\"1 = Completely wrong or irrelevant\"\n",
    "        f\"Important rules:\"\n",
    "        f\"- Respond with ONLY the integer rating (1, 2, 3, 4, or 5).\"\n",
    "        f\"- Do NOT provide explanations, reasoning, or extra words.\"\n",
    "        f\"original prompt: {format_input(entry)}\"\n",
    "        f\"correct response: {entry['output']}\"\n",
    "        f\"generated response: {entry[json_key]}\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15213e3-c537-479e-ab05-849d0dff3e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
